


# What the hell is this knowlege? 
What a question. But lets start out small. I assume that in order to understand what knowlege is, I need to start out be defining information first.

> "In the nights when I cannot sleep, thoughts crowd into my mind ...
> Whence and how do they come? I do not know and I have nothing to do with it. Those which please me, I keep in my head an hum them."



But what is information? For this question, many different concepts and theories exist. I'll try to create a helpful overview by listing various well established definitions before defining a working definition in this project.
Information is a classical polysemantic word, where it's semantic meaning is largely dependent the given perspective. 
Hence I want to list the, to my understanding, most common perspectives on Information:

**Oxford Dictionary**
According to the Oxford dictionary, Information can be defined as "*facts or details about somebody/something*"


**Philosophical Perspective:** <br>
Strongly related with notions such as reference, meaning and representation: semantic information has intentionality −“aboutness”−, it is directed to other things.


**Scientific Perspective**: <br>
Problems are expressed in terms of a notion of information amenable to quantification.

**Mathematical Persepective**:
<br/><br/>
    - Fisher Information:<br>
      Measures the dependence of a random variable X on an unknown parameter θ upon which the probability of X depends
  <br/><br/>
    - Algorithmic information:<br> 
      Measures the length of the shortest program that produces a string on a universal Turing machine. 
  <br/><br/>
    - von Neumann Entropy<br>
      Gives a measure of the quantum resources necessary to faithfully encode the state of the source-system.
      <br/><br/>
    - Shannon Entropy:<br> is concerned with the statistical properties of a given system and the correlations between the states of two systems, independently of the meaning and any semantic content of those states. 



## But why should we care?  


### 1. Communication
Based on my limitied experience and time on this plant, I was that a fundamental need of all (or lets say maybe most) humans, is to communicate with others, to share, expresse and partake in the very existance of each other.
Besides the aspects of needs, communication also has a very pratical aspect to itself: It helps us, to navigate through this world, to reach our goals and to eventually continue our survival. 

#### Shannon's Communication/Information Theory
In this theory, information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, 
and to have the receiver reconstruct the message with low probability of error, in spite of the channel noise.

According to Shannon (1948; see also Shannon and Weaver 1949), a general
communication system consists of five parts:

- A source S, which generates the message to be received at the destination.

- A transmitter T, which turns the message generated at the source into a signal to be transmitted. In the cases in which the information is encoded, encoding is also implemented by this system.

-  A channel CH, that is, the medium used to transmit the signal from the transmitter to the
receiver.

- A receiver R, which reconstructs the message from the signal.

- A destination D, which receives the message. 
<br><br>


In his proposed theory, he also proposed the following definitions: 


**Binary Digits (Bits)**
Shannon introduced the concept of binary digits, or bits, as the fundamental unit of information. 
A bit is a binary digit that can take on one of two values, typically 0 or 1. This concept revolutionized the way information was quantified and transmitted.


**Information as a Decrease in Uncertainty**
Information is defined as a decrease in uncertainty. For example, if Bob is trying to guess which shape Alice is holding, and Alice tells him it is blue, this reduces the set of possible shapes, thereby decreasing Bob's uncertainty.

**Entropy**
Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. It is measured by the formula

$$H(X) := -\sum_{x \in X} p(x) \log p(x)$$


where $H(X)$ can be seen as a degree of suprise, or spoken very causually, might be seen as the number of yes/no question one needs to ask (and get answered) to obtain a certain message)

Maybe the word *Entropy* rings a bell here
**Link from Physics to Informationtheory & Entropy**
To quote from [Maxwell and his deamon](https://www.ias.ac.in/public/Volumes/reso/015/06/0548-0560.pdf):

> Moral. The 2nd law of thermodynamics has the same degree of truth as the statement that if you throw a tumberflu of water into the sea, you cannot ge tthe same tumblerflu of water out again


which led to physicists speaking about micro- and macrostates where entropy became a physical equivalent of probabilty: The entropy of a given macro state is the logarithm of the number of possbiel micro-states.

More on that can be found here: [Maxwells Daemon](https://www.spektrum.de/lexikon/physik/maxwellscher-daemon/9530)

This daemon led Szilárd to close the loop leading to shannons conception of entropy by establishing that every time, maxwells daemon had to make a (particel) decsion, it costs the daemon a certain "something", which can be defined as *Information*.

So eventually, it is all one problem. 
To reduce entropy in a box of gas, to perfom useful work, one pays the price of information.


Amazing.


<img src="../assets/images/escher_1.jpeg" alt="Alt text">




But what's behind this? What is it that we want to convey?
This theory already works with a message, that is supposed to be sent/transmitted. But what is this meassage and how can we understand it's meaning? 
For this question, I'd like to take *language* as my vehicle of investigation. More concretely, the *Theory of meaning in languages*
As an introduction, I want to share a very interesting idea: 

 [Edward Gibson](https://bcs.mit.edu/directory/edward-gibson) prosed an interesting idea, namely that human language is constructed by us humans via words/sentences and serves as a tool to communicate with our fellows about things that are important to us. At least to me, this is very fascinating and led me to question wether you can identify what things are important to certain groups by comparing the relativ amount of descriptive words per topic that their languages contain? 
Apparently, across different language families, one can see structural patterns in word specifications per objects. For example, he claimed that in more tribal societies, the number speakable colour-categories is way smaller that in more modern (capitalistic) societies.

