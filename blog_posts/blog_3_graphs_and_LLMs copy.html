

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title>Dominik Pichler</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico"/>
    <link href="../css/styles-reduced.css" rel="stylesheet"/>
    <link href="../css/dp.css" rel="stylesheet"/>
    <link href="../css/animations.css" rel="stylesheet"/>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



</head>
<body id="page-top">
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="index.html">

            <svg width="100" viewBox="0 0 168 96" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path fill-rule="evenodd" clip-rule="evenodd"
                      d="M2.048 56.416C0.682667 59.1893 0 62.4107 0 66.08C0 67.4458 0.0945791 68.7525 0.283737 70H11.7822C11.3514 68.8534 11.136 67.5467 11.136 66.08C11.136 63.3493 11.8827 61.216 13.376 59.68C14.912 58.1013 16.768 57.312 18.944 57.312C21.12 57.312 22.9547 58.1013 24.448 59.68C25.984 61.2587 26.752 63.4133 26.752 66.144C26.752 67.5911 26.5363 68.8764 26.1049 70H37.696V36.64H26.752V53.216C25.6427 51.552 24.1067 50.2293 22.144 49.248C20.224 48.2667 18.0907 47.776 15.744 47.776C12.8 47.776 10.1333 48.5227 7.744 50.016C5.35467 51.5093 3.456 53.6427 2.048 56.416ZM80.2234 70C80.4461 68.7718 80.5575 67.4865 80.5575 66.144C80.5575 62.4747 79.7468 59.2533 78.1255 56.48C76.5042 53.664 74.2855 51.5093 71.4695 50.016C68.6535 48.5227 65.4962 47.776 61.9975 47.776C58.4988 47.776 55.3415 48.5227 52.5255 50.016C49.7095 51.5093 47.4908 53.664 45.8695 56.48C44.2482 59.2533 43.4375 62.4747 43.4375 66.144C43.4375 67.4912 43.5415 68.7765 43.7494 70H55.1056C54.7509 68.8865 54.5735 67.6012 54.5735 66.144C54.5735 63.2427 55.2775 61.0453 56.6855 59.552C58.0935 58.016 59.8642 57.248 61.9975 57.248C64.0882 57.248 65.8375 58.016 67.2455 59.552C68.6962 61.088 69.4215 63.2853 69.4215 66.144C69.4215 67.6012 69.2275 68.8865 68.8395 70H80.2234ZM97.3075 70H86.3635V48.288H97.3075V52.768C98.4168 51.2747 99.8675 50.1013 101.66 49.248C103.451 48.352 105.478 47.904 107.74 47.904C110.428 47.904 112.817 48.48 114.908 49.632C117.041 50.784 118.705 52.4267 119.9 54.56C121.137 52.5973 122.822 50.9973 124.956 49.76C127.089 48.5227 129.414 47.904 131.932 47.904C136.369 47.904 139.889 49.248 142.492 51.936C145.137 54.624 146.46 58.3573 146.46 63.136V70H135.579V64.608C135.579 62.304 134.961 60.5333 133.724 59.296C132.529 58.016 130.865 57.376 128.732 57.376C126.598 57.376 124.913 58.016 123.676 59.296C122.481 60.5333 121.883 62.304 121.883 64.608V70H111.004V64.608C111.004 62.304 110.385 60.5333 109.147 59.296C107.953 58.016 106.289 57.376 104.156 57.376C102.022 57.376 100.337 58.016 99.0995 59.296C97.9048 60.5333 97.3075 62.304 97.3075 64.608V70Z"
                      fill="#85FF9F"/>
                <path d="M157.296 70.512C155.376 70.512 153.797 69.9573 152.56 68.848C151.365 67.696 150.768 66.288 150.768 64.624C150.768 62.9173 151.365 61.488 152.56 60.336C153.797 59.184 155.376 58.608 157.296 58.608C159.173 58.608 160.709 59.184 161.904 60.336C163.141 61.488 163.76 62.9173 163.76 64.624C163.76 66.288 163.141 67.696 161.904 68.848C160.709 69.9573 159.173 70.512 157.296 70.512Z"
                      fill="#85FF9F"/>
            </svg>
        </a>

        <div>


        </div>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">


                <!--  <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li> -->
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="index.html">Home</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="blog.html">Blog</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
            </ul>
        </div>
    </div>
</nav>
</nav>


<section class="container h-100 masthead_content">
    <div class="text-blog">
        <div class="textbox-blog blog-text">

          <h2 class="code-line" data-line-start=0 data-line-end=1 ><a id="Introduction_0"></a>Introduction</h2>
          <p class="has-line-data" data-line-start="1" data-line-end="3">Connecting semantically rich, formally defined linked data  and large language models looks like a lot fun.<br>
          This “fun” currently (1/2025) allows for multiple interesting use-cases, for example the currenlty most prominent ones:</p>
          <ul>
          <li class="has-line-data" data-line-start="3" data-line-end="4">KG-enhanced LLMs</li>
          <li class="has-line-data" data-line-start="4" data-line-end="5">LLM-augmented KGs</li>
          <li class="has-line-data" data-line-start="5" data-line-end="7">Synergized LLMs + KGs</li>
          </ul>
          <p class="has-line-data" data-line-start="7" data-line-end="8">For this investiagion, the following is primarily based on <em>Pan <a href="http://et.al">et.al</a> (2024)</em><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
          <h2 class="code-line" data-line-start=10 data-line-end=11 ><a id="Overview_10"></a>Overview</h2>
          <p class="has-line-data" data-line-start="11" data-line-end="16">Despite their success in many applications, LLMs have<br>
          been criticized for their lack of factual knowledge.<br>
          However, further studies reveal<br>
          that LLMs are not able to recall facts and often experience<br>
          hallucinations by generating statements that are factually incorrect.</p>
          <p class="has-line-data" data-line-start="17" data-line-end="26">As black-box models, LLMs are also criticized for their<br>
          lack of interpretability. LLMs represent knowledge implicitly<br>
          in their <a href="http://parameters.It">parameters.It</a> is difficult to interpret or validate<br>
          the knowledge obtained by LLMs.<br>
          Even though some LLMs are equipped to explain their predictions by applying<br>
          chain-of-thought, their reasoning explanations also suffer<br>
          from the hallucination issue.<br>
          This severely impairs the application of LLMs in high-stakes scenarios, such as<br>
          medical diagnosis and legal judgment.</p>
          <p class="has-line-data" data-line-start="27" data-line-end="31">To address the above issues, a potential solution is to incorporate<br>
          knowledge graphs (KGs) into LLMs. Knowledge<br>
          graphs (KGs), storing enormous facts in the way of triples,<br>
          i.e., (head entity, relation, tail entity).</p>
          <p class="has-line-data" data-line-start="33" data-line-end="39">Nevertheless, KGs are difficult to construct automatically according to Zhong et. al (2023) <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> , and<br>
          current approaches in KGs [27], [33], [34] are inadequate<br>
          in handling the incomplete and dynamically changing nature<br>
          of real-world KGs. These approaches fail to effectively<br>
          model unseen entities and represent new facts. In addition,<br>
          they often ignore the abundant textual information in KGs.</p>
          <p class="has-line-data" data-line-start="40" data-line-end="43">Therefore, it is also necessary to utilize LLMs to address the<br>
          challenges faced in KGs so maybe LLMs and KGs can work in Synergy.<br>
          Hence I want to investigate a potential roadmap for uniting the powers of LLMs and KGs.</p>
          <p class="has-line-data" data-line-start="45" data-line-end="46">For this purpose, Knowledge Graphs can be categorized into the following groups:</p>
          <ul>
          <li class="has-line-data" data-line-start="46" data-line-end="47">Encyclopedic Knowledge Graphs</li>
          <li class="has-line-data" data-line-start="47" data-line-end="48">Common Sense Knowledge Graphs</li>
          <li class="has-line-data" data-line-start="48" data-line-end="49">Domain specific Knowledge Graphs</li>
          <li class="has-line-data" data-line-start="49" data-line-end="50">Multi-modal Knowledge Graphs</li>
          </ul>
          <h3 class="code-line" data-line-start=53 data-line-end=54 ><a id="KGenhanced_LLMs_53"></a>KG-enhanced LLMs</h3>
          <p class="has-line-data" data-line-start="54" data-line-end="55">How can we make use of the large, formally structured knowledge bases of knowledge graphs in LLMs? Here, researchers have proposed different approaches:</p>
          <ol>
          <li class="has-line-data" data-line-start="56" data-line-end="57">Incorporate KGs into the pre-training stage of LLMs</li>
          <li class="has-line-data" data-line-start="57" data-line-end="58">Incorporate KGs into the inference stage of LLMs</li>
          <li class="has-line-data" data-line-start="58" data-line-end="59">Utilize KGs to investigate and improve the interpretability of LLMs</li>
          </ol>
          <h3 class="code-line" data-line-start=61 data-line-end=62 ><a id="LLMaugmented_KGs_61"></a>LLM-augmented KGs</h3>
          <p class="has-line-data" data-line-start="62" data-line-end="63">While not only LLMs can benefit from the the large corpus of formally defined factual knowledge of KGs, KGs can also benefit from the learned semantic text understanding of LLMs. Hence LLMs can be used to address KG-related tasks.</p>
          <p class="has-line-data" data-line-start="64" data-line-end="65">Here researches identified and proposed different approaches:</p>
          <ol>
          <li class="has-line-data" data-line-start="66" data-line-end="71">
          <p class="has-line-data" data-line-start="66" data-line-end="70"><strong>LLM-augmented KG embedding</strong> includes studies that<br>
          apply LLMs to enrich representations of KGs by<br>
          encoding the textual descriptions of entities and<br>
          relations.</p>
          </li>
          <li class="has-line-data" data-line-start="71" data-line-end="75">
          <p class="has-line-data" data-line-start="71" data-line-end="74"><strong>LLM-augmented KG completion</strong> includes papers that<br>
          utilize LLMs to encode text or generate facts for<br>
          better KGC performance.</p>
          </li>
          <li class="has-line-data" data-line-start="75" data-line-end="79">
          <p class="has-line-data" data-line-start="75" data-line-end="79"><strong>LLM-augmented KG construction</strong> includes works that<br>
          apply LLMs to address the entity discovery, coreference<br>
          resolution, and relation extraction tasks for KG<br>
          construction.</p>
          </li>
          <li class="has-line-data" data-line-start="79" data-line-end="83">
          <p class="has-line-data" data-line-start="79" data-line-end="82"><strong>LLM-augmented KG-to-text Generation</strong> includes research<br>
          that utilizes LLMs to generate natural language<br>
          that describes the facts from KGs.</p>
          </li>
          <li class="has-line-data" data-line-start="83" data-line-end="87">
          <p class="has-line-data" data-line-start="83" data-line-end="87"><strong>LLM-augmented KG question</strong> answering includes studies<br>
          that apply LLMs to bridge the gap between<br>
          natural language questions and retrieve answers<br>
          from KGs.</p>
          </li>
          </ol>
          <h2 class="code-line" data-line-start=90 data-line-end=91 ><a id="KGenhanced_LLM_Inference_90"></a>KG-enhanced LLM Inference</h2>
          <p class="has-line-data" data-line-start="91" data-line-end="94">In this application, the knowledge space and the text space are seperated and the knowledge gets injected during the inference.<br>
          Due to this approach, it is mostly focused on QA Tasts as in this case, both an accurate caption of the semantic meanings as well as up-to-date real-world knowlege are required.<br>
          Thereby, this approach places itself in a cohort of information retrival systems like traditional ranker, neural rerankers, traditional RAG and many others.</p>
          <p class="has-line-data" data-line-start="95" data-line-end="96">KG-enhanced LLM Inference typically comes in the following flavors:</p>
          <h4 class="code-line" data-line-start=97 data-line-end=98 ><a id="1_RetrievalAugmented_Knowledge_Fusion_97"></a>1. Retrieval-Augmented Knowledge Fusion</h4>
          <p class="has-line-data" data-line-start="98" data-line-end="100">Traditional RAG<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> proposes a combination of non-parametric and parametric moduls to handle the (large) external corpus of knowledge.<br>
          Given the input text, RAG first searches for relevant Knowledge in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables z and feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context information.</p>
          <p class="has-line-data" data-line-start="102" data-line-end="103">But why not stick to the basline RAG? Whats the problem?</p>
          <p class="has-line-data" data-line-start="104" data-line-end="105">Well, consider the following question: <em>“What name was given to the son of the man who defeated the usurper Allectus?”</em></p>
          <p class="has-line-data" data-line-start="106" data-line-end="107">A baseline RAG would generally follow these steps to answer this question:</p>
          <ol>
          <li class="has-line-data" data-line-start="108" data-line-end="109"><strong>Identify the Man</strong>: Determine who defeated Allectus.</li>
          <li class="has-line-data" data-line-start="109" data-line-end="110"><strong>Research the Man’s Son</strong>: Look up information about this person’s family, specifically his son.</li>
          <li class="has-line-data" data-line-start="110" data-line-end="112"><strong>Find the Name</strong>: Identify the name of the son</li>
          </ol>
          <p class="has-line-data" data-line-start="112" data-line-end="117">The challenge usually arises at the first step because a baseline RAG retrieves text based on semantic similarity, not directly answering complex queries where specific details may not be explicitly mentioned in the dataset!<br>
          KG-enhanced LLM Inferences on the contrairy proposes to use KG-based Story-fragments further improves architecture by adding an additional<br>
          module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated<br>
          long stories. For example KGLM  selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> could describe facts using out-of domain.<br>
          words or phrases.</p>
          <p class="has-line-data" data-line-start="118" data-line-end="119">So to summarize, RAG works well when answers are contained locally within regions of text. But what if the answers are distributed across multiple text fragments or even documents (so called <em>Query-focused abstractive summarization over an entire corpus</em>)? This is a situation in which Graph-based RAG comes to the rescue. It</p>
          <h4 class="code-line" data-line-start=122 data-line-end=123 ><a id="Graphbased_RAG_122"></a>Graph-based RAG</h4>
          <p class="has-line-data" data-line-start="123" data-line-end="125">Unlike a baseline RAG that uses a vector database to retrieve semantically similar text, GraphRAG enhances RAG by incorporating knowledge graphs (KGs).<br>
          But how does it work?</p>
          <p class="has-line-data" data-line-start="127" data-line-end="128">Fundamentally, Graph RAG consists of two processes:</p>
          <ul>
          <li class="has-line-data" data-line-start="128" data-line-end="129">Indexing - concerned with building and indexing the fundamental knowledge base</li>
          <li class="has-line-data" data-line-start="129" data-line-end="131">Querying - concernd with the actual User/Agent Query</li>
          </ul>
          <p class="has-line-data" data-line-start="131" data-line-end="135">But what does that mean?<br>
          To beginn with, (Graph) RAG Systems usually start out with a large® corpus of more or less factual knowledge that should be incorporated in the QA Reply from an LLM.<br>
          So, this corpus initially has to be structured in a way that makes sure that the LLM can access the factual knowledge, process it and utilize it in its replys/answers.<br>
          This is usually done in the index phase that consists of four steps:</p>
          <h4 class="code-line" data-line-start=137 data-line-end=138 ><a id="Index_Phase_137"></a>Index Phase</h4>
          <ol>
          <li class="has-line-data" data-line-start="138" data-line-end="141">Source Documents -&gt; Text Chunks<br>
          While large corpus are great for humans with large spans of concentration, the same does typically not hold true for LLMs due to limited context window.<br>
          Hence, the originally large corpus has to be broken down into the smallest analyzable units of text, in order to extract more detailed, formally defined knowledge / information.</li>
          </ol>
          <ol start="2">
          <li class="has-line-data" data-line-start="143" data-line-end="145">Text Chunks -&gt; Element Instances -&gt; Element Summaries<br>
          In modern approaches, especially in natural lanuage systems, Knowledge consists mostly out of the information present about extisting entities and their relationships. In Graph RAG Systems, those ERs are forming the building block of the knowledge graphs. What is important to note, is that in the original Graph RAG paper<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> by <em>Edge et. al (2024)</em> LLMs are used to extract ERs. It was observed that the number of entity references detected was to a large degree dependent on the level of text unit segmentation. This use of an LLM to somewhat extract ERs is already a kind of abstract summary. An alternative to this LLM based extraction might be ER-Detection Systems that rely more on the grammar itself, but as far as I know, those sysmtes tend to be very labor intensive and while delivering a very high precision, the recall of such systems tends to be very low. But I am very curious about what the future holds and wether approaches such as Chomskys concept of universal grammar might be helpful in increasing the summarization erorrs and bias of LLMs for this task.</li>
          </ol>
          <p class="has-line-data" data-line-start="147" data-line-end="149">After this, another round of LLM summarization is applied with the goal of converting all instance level summaries into single blocks of descriptive test for each graph element.<br>
          (Entity Node, Relationship Edge, Claim Covariate)</p>
          <p class="has-line-data" data-line-start="150" data-line-end="156">A potential concern at this stage is that the LLM may not consistently extract references to the<br>
          same entity in the same text format, resulting in duplicate entity elements and thus duplicate nodes<br>
          in the entity graph. However, since all closely-related “communities” of entities will be detected<br>
          and summarized in the following step, and given that LLMs can understand the common entity<br>
          behind multiple name variations, this approach is resilient to such variations provided there<br>
          is sufficient connectivity from all variations to a shared set of closely-related entities</p>
          <ol start="3">
          <li class="has-line-data" data-line-start="158" data-line-end="166">
          <p class="has-line-data" data-line-start="158" data-line-end="165">Element Summaries -&gt; Graph Communities<br>
          The index created in the previous step can be modelled as an homogeneous undirected weighted<br>
          graph in which entity nodes are connected by relationship edges, with edge weights representing the<br>
          normalized counts of detected relationship instances.<br>
          Given such a graph, a variety of community detection algorithms may be used to partition the graph into communities of nodes with stronger<br>
          connections to one another than to the other nodes in the graph.<br>
          To my understanding, the <em>Leiden Algorithm</em> is particularly useful for this purpose.</p>
          </li>
          <li class="has-line-data" data-line-start="166" data-line-end="169">
          <p class="has-line-data" data-line-start="166" data-line-end="169">Graph Communities -&gt; Community Summaries<br>
          Given communities have been identified, summaries for each commity can be generated. Those summaries can not only be useful for Graph RAG but also in general for understanding the global structure and semantics of a dataset.<br>
          For example, a user may scan through community summaries at one level looking for general themes of interest, then follow links to the reports at the lower level that provide more details for each of the subtopics.</p>
          </li>
          </ol>
          <h2 class="code-line" data-line-start=172 data-line-end=173 ><a id="TLDR_The_Graph_In_Graph_RAG_172"></a>TLDR; The Graph In Graph RAG</h2>
          <p class="has-line-data" data-line-start="173" data-line-end="175">So as far as I understand and put simply:<br>
          Entities and relationships and claims, are extracted and organized in a graph-structure. Then, for each entity, summaries are generated. Then based on this entity summaries (with relationsships that capture the normalzied count of detected relationsship instances, graph communities are identified and equipt with a community summary.</p>
          <p class="has-line-data" data-line-start="177" data-line-end="178">What still remains a big question to me is how the entity summaries are generated, how logical reasoning could come into play and how the graph communities are being formed based on the entity summary graph (<em>Leiden Algorithm)</em></p>
          <h2 class="code-line" data-line-start=180 data-line-end=181 ><a id="When_is_GraphRAG_particularly_useful_180"></a>When is Graph-RAG particularly useful?</h2>
          <p class="has-line-data" data-line-start="181" data-line-end="182">GraphRAG is intended to support critical information discovery and analysis use cases where the information required to arrive at a useful insight spans many documents, is noisy, is mixed with mis and/or dis-information, or when the questions users aim to answer are more abstract or thematic than the underlying data can directly answer.</p>
          <p class="has-line-data" data-line-start="183" data-line-end="185">GraphRAG is designed to be used in settings where users are already trained on responsible analytic approaches and critical reasoning is expected. GraphRAG is capable of providing high degrees of insight on complex information topics, however human analysis by a domain expert of the answers is needed in order to verify and augment GraphRAG’s generated responses.<br>
          GraphRAG is intended to be deployed and used with a domain specific corpus of text data. GraphRAG itself does not collect user data, but users are encouraged to verify data privacy policies of the chosen LLM used to configure GraphRAG.</p>
          <h4 class="code-line" data-line-start=188 data-line-end=189 ><a id="2_Knowledge_Graph_Prompting_188"></a>2. Knowledge Graph Prompting</h4>
          <p class="has-line-data" data-line-start="189" data-line-end="190">left out for now</p>
          <h2 class="code-line" data-line-start=193 data-line-end=194 ><a id="KGLM__Using_KGs_for_FactAwareLanguage_Modelling_193"></a>KGLM - Using KGs for Fact-Aware-Language Modelling</h2>
          <p class="has-line-data" data-line-start="194" data-line-end="201">As described before, modeling human language requires the ability<br>
          to not only generate fluent text but also encode factual knowledge. However, traditional<br>
          language models are only capable of remembering facts seen at training time, and often<br>
          have difficulty recalling them. Here is where KGLM comes to play:<br>
          The KGLM maintains a dynamically growing local knowledge graph, a subset of the knowledge graph that contains entities that have already been mentioned in the text, and their related entities. When generating entity tokens, the model either decides to render<br>
          a new entity that is absent from the local graph, thereby growing the local knowledge graph, or to<br>
          render a fact from the local graph.</p>
          <p class="has-line-data" data-line-start="203" data-line-end="207">Initially, the graph is empty and the model uses the entity Super Mario Land to render the first three<br>
          tokens, thus adding it and its relations to the local knowledge graph. After generating the next two tokens (“is”, “a”) using the standard language model,<br>
          the model selects Super Mario Land as the parententity, Publication Date as the relation to render, and copies one of the tokens of the date entity as<br>
          the token (“1989” in this case).</p>
          <h2 class="code-line" data-line-start=210 data-line-end=211 ><a id="Summary_210"></a>Summary</h2>
          <p class="has-line-data" data-line-start="212" data-line-end="213">So to my understanding, integrating factual knowledge into LLMs can happen via:</p>
          <ul>
          <li class="has-line-data" data-line-start="213" data-line-end="214">
          <p class="has-line-data" data-line-start="213" data-line-end="214">RAG - Where documents are stored in pieces and assigned to queries via similarity of encodings</p>
          </li>
          <li class="has-line-data" data-line-start="214" data-line-end="216">
          <p class="has-line-data" data-line-start="214" data-line-end="215">Graph RAG - Where documents are stored in pieces</p>
          </li>
          <li class="has-line-data" data-line-start="216" data-line-end="218"></li>
          </ul>
          <hr>
          <h4 class="code-line" data-line-start=219 data-line-end=220 ><a id="Source_219"></a>Source:</h4>
          <hr class="footnotes-sep">
          <section class="footnotes">
          <ol class="footnotes-list">
          <li id="fn1"  class="footnote-item"><p class="has-line-data" data-line-start="220" data-line-end="221">S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang and X. Wu, “Unifying Large Language Models and Knowledge Graphs: A Roadmap,” in IEEE Transactions on Knowledge and Data Engineering, vol. 36, no. 7, pp. 3580-3599, July 2024, doi: 10.1109/TKDE.2024.3352100. keywords: {Task analysis;Decoding;Cognition;Training;Predictive models;Knowledge graphs;Chatbots;Natural language processing;large language models;generative pre-training;knowledge graphs;roadmap;bidirectional reasoning}, <a href="#fnref1" class="footnote-backref">↩</a></p>
          </li>
          <li id="fn2"  class="footnote-item"><p class="has-line-data" data-line-start="221" data-line-end="222">Zhong, L., Wu, J., Li, Q., Peng, H., &amp; Wu, X. (2023). <em>A Comprehensive Survey on Automatic Knowledge Graph Construction</em>. arXiv. <a href="https://arxiv.org/abs/2302.05019">https://arxiv.org/abs/2302.05019</a> <a href="#fnref2" class="footnote-backref">↩</a></p>
          </li>
          <li id="fn3"  class="footnote-item"><p class="has-line-data" data-line-start="225" data-line-end="228">P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,<br>
          H. Kuttler, M. Lewis, W.-t. Yih, T. Rockt ¨ aschel, S. Riedel, ¨<br>
          and D. Kiela, “Retrieval-augmented generation for knowledgeintensive nlp tasks,” in NeurIPS, vol. 33, 2020, pp. 9459–9474. <a href="#fnref3" class="footnote-backref">↩</a></p>
          </li>
          <li id="fn4"  class="footnote-item"><p class="has-line-data" data-line-start="229" data-line-end="232">R. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh,<br>
          “Barack’s wife hillary: Using knowledge graphs for fact-aware<br>
          language modeling,” in ACL, 2019, pp. 5962–5971 <a href="#fnref4" class="footnote-backref">↩</a></p>
          </li>
          <li id="fn5"  class="footnote-item"><p class="has-line-data" data-line-start="234" data-line-end="236">Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., &amp; Larson, J. (2024). From local to global: A graph RAG approach to query-focused summarization. arXiv. <a href="https://arxiv.org/abs/2404.16130">https://arxiv.org/abs/2404.16130</a><br>
          MLA Style <a href="#fnref5" class="footnote-backref">↩</a></p>
          </li>
          </ol>
          </section>
</section>





<section id="contact">
    <div class="flexbox-contact container">
        <div class="col-lg-6 col-md-12 col-sm-12 contact-container">
            <div class="contact-text">
                <h2> Get in touch</h2>
                <p>
                    Feel free to send me a message anytime.<br>
                    I'd love to talk about you and your projects.
                </p>
            </div>
            <div class="contact-details">

                <ul class="contact-list">
                    <li style="color: rgb(47, 59, 76">
                        <i class="fab fa-github favicon_contact" ></i>
                        <a target="_blank" style="color: rgb(47, 59, 76)" href="https://github.com/dominik-pichler">@dominik-pichler
                        </a>
                    </li>
                    <li style="color: rgb(47, 59, 76">
                        <i class="fab fa-linkedin-in favicon_contact">
                        </i>
                        <a target="_blank" style="color: rgb(47, 59, 76)"
                           href="https://linkedin.com/in/dominik-pichler-811827135/">Dominik Pichler
                        </a>
                    </li>
                    <li style="color: rgb(47, 59, 76">
                        <i class="fas fa-envelope favicon_contact">
                        </i>
                        <a target="_blank" style="color: rgb(47, 59, 76)"
                           href="mailto:info@dominik-pichler.com">
                            info[at]dominik-pichler[dot]com
                        </a>
                    </li>
                    <li style="color: rgb(47, 59, 76)" ><i class="fas fa-map-marker-alt favicon_contact"></i>   1140 Vienna / 4020 Linz - Austria</li>


                </ul>

            </div>


        </div>

        <div class="col-md-6 col-sm-12">
            <img class="contact_image" src="../assets/img/Schiele_2.png" alt="description of the image">
        </div>


    </div>
</section>


<!-- Footer-->
<footer class="footer small text-center text-white-50">
    <div class="container">Copyright © Dominik Pichler
    </div>
</footer>
<!-- Bootstrap core JS-->
<script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
<!-- Third party plugin JS-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
<!-- Core theme JS-->
<script src="js/scripts.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/validate.js/0.13.1/validate.min.js"></script>
<link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i"
      rel="stylesheet"/>
<script src="https://cdn.jsdelivr.net/npm/animejs@3.0.1/lib/anime.min.js"></script>
<script src="js/animations.js">
</script>

</body>
</html>
